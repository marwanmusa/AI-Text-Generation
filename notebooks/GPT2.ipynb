{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Sentences\n",
    "sentence = \"What is life\"\n",
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and Decode Text\n",
    "output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2061,   318,  1204,    30,   198,   198, 14662,   318,   257,  7002,\n",
       "            11,   407,   257, 10965,    13,   632,   338,   546,   262,  1310,\n",
       "          1243,   287,  1204,   326,   787,   262,  1263,  1243,  2861,   340,\n",
       "            13,   383,  1243,   345,   466,   329,  1854,    11,   262,   661,\n",
       "           345,  1826,    11,   290,   262,  6461,   345,   423,  1863,   262,\n",
       "           835,    13,   921,   836,   470,   423,   284,   307,  5863,   393,\n",
       "          5527,   284,   423,   257,  1049,  1204,    11,   475,   852,  2695,\n",
       "           351,   508,   345,   389,   290,   644,   345,  1053,  1392,  1016,\n",
       "           329,   345,   318,   262,  1266,   835,   284,  3067,   262,   995,\n",
       "           290,   787,   257,  3580,   287,   262,  3160,   286,  1854,    13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2061)\n",
      "What\n"
     ]
    }
   ],
   "source": [
    "# Decode example\n",
    "print(output[0][0]) # in tensor form\n",
    "print(tokenizer.decode(output[0][0])) # decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is life?\n",
      "\n",
      "Life is a journey, not a destination. It's about the little things in life that make the big things worth it. The things you do for others, the people you meet, and the experiences you have along the way. You don't have to be famous or rich to have a great life, but being content with who you are and what you've got going for you is the best way to travel the world and make a difference in the lives of others.\n"
     ]
    }
   ],
   "source": [
    "# Output Result\n",
    "text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
